{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 1. THE REINFORCEMENT LEARNING PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1: Self-Play** <br>\n",
    "\n",
    "Suppose, instead of playing against a random\n",
    "opponent, the reinforcement learning algorithm described above played against\n",
    "itself. What do you think would happen in this case? Would it learn a different\n",
    "way of playing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously assumed the RL algorithm would train against an \"imperfect player, one whose play is sometimes incorrect and allows us to win\" because a skilled player can play as to never lose. We also consider that the draws and losses are equally bad for the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the RL agent played against itself, as it gradually learns to play the optimal way, at one point the agent would learn how not to lose. <br> \n",
    "After this milestone, the agent would always get draws & losses as they are equally bad for the agent, and thus would never be able to improve <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2: Symmetries**\n",
    "\n",
    "Many tic-tac-toe positions appear different but\n",
    "are really the same because of symmetries. How might we amend the reinforcement learning algorithm described above to take advantage of this? In what\n",
    "ways would this improve it? Now think again. Suppose the opponent did not\n",
    "take advantage of symmetries. In that case, should we? Is it true, then, that\n",
    "symmetrically equivalent positions should necessarily have the same value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| X | O | X |   | X | O | X |   | - | O | X |   | X | - | - |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| O | - | - |   | - | - | O |   | - | - | O |   | O | - | O |\n",
    "| X | O | - |   | - | O | X |   | X | O | X |   | X | O | X |\n",
    "\n",
    "are 4 symmetric equivalent positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent could take advantage of the symmetric positions by considering them as forming only one \"super-state\", thus reducing the number of possible states, and improving the speed of learning the value associated with that \"super-state\". <br>\n",
    "If both the agent and opponent don't take advantage of symmetries, then the learned value associated with each of the symmetric positions would be different. <br>\n",
    " Since the agent learns to beat its opponent, but not necessarily to play the optimal way, then that means each symmetric position should have a different value that is adapted to the behaviour of the opponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.3: Greedy Play**\n",
    "\n",
    " Suppose the reinforcement learning player was\n",
    "greedy, that is, it always played the move that brought it to the position that\n",
    "it rated the best. Would it learn to play better, or worse, than a nongreedy\n",
    "player? What problems might occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, the agent would always lean towards the same moves every game. If it was biased in the wrong \"path\" at the beginning, then it would play bad all the time until it finally learned that this \"wrong path\" is, in fact, wrong (i.e. the associated values are not optimal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.4: Learning from Exploration**\n",
    "\n",
    "Suppose learning updates occurred\n",
    "after all moves, including exploratory moves. If the step-size parameter is\n",
    "appropriately reduced over time, then the state values would converge to a\n",
    "set of probabilities. What are the two sets of probabilities computed when we\n",
    "do, and when we do not, learn from exploratory moves? Assuming that we\n",
    "do continue to make exploratory moves, which set of probabilities might be\n",
    "better to learn? Which would result in more wins?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two sets of probabilities :\n",
    "- The set of \"greedy probabilities\", which corresponds to the states visited a lot and that a high state value.\n",
    "- The set of \"exploratory\" states, which corresponds to less visited states for which the estimation of the value is approximative\n",
    "\n",
    "It is better to learn the set of exploratory states, but we would sacrifice wins for better value estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.5: Other Improvements**\n",
    "\n",
    "Can you think of other ways to improve\n",
    "the reinforcement learning player? Can you think of any better way to solve\n",
    "the tic-tac-toe problem as posed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore more at the beginning, exploit more at the end\n",
    "- Losses should have a worse reward than Draws\n",
    "- Consider symmetric positions as one, for both the agent and the opponent, therefore accelerating the learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
