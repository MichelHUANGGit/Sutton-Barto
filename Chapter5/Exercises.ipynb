{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.1**\n",
    "\n",
    "Consider the diagrams on the right in Figure 5.1. Why does the estimated value function jump up for the last two rows in the rear ? Why does it drop off for the whole last row on the left? Why are the frontmost values higher in the upper diagrams than in the lower?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value jumps up because with a sum of 20 or 21, the player's chances of winning are high and he should obviously not hit. <br>\n",
    "They drop off for the row on the left (when the dealer has an Ace) because the Ace is a good card. It's like a safety layer when you hit and the sum exceeds 21 you can retry. <br>\n",
    "The frontmost values are higher in the upper diagrams than in the lower because of the same reason, having a usable ace is a safety layer and is better than not having one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.2**\n",
    "\n",
    "Suppose every-visit MC was used instead of first-visit MC on the blackjack task. Would you expect the results to be very different? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we defined the states being the sum of the agent's cards, and one of the dealer's card, makes the case when we encounter the same state twice in one episode very rare. For instance, the sum of the agent's cards can only increase throughout the game. The very rare case only happens when there is a usable ace (example: the agent has an ace and a 2, adding up to 13. He hits and gets a face card, adding +10 and ends at 23. He makes his ace count as 1, so he's back at 13.) <br>\n",
    "So in Blackjack, first-visit MC and every-visit MC are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.3**\n",
    "\n",
    "What is the backup diagram for Monte Carlo estimation of $q_\\pi$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.4**\n",
    "\n",
    "The pseudocode for Monte-Carlo ES is inefficient because for all state-action pairs, it maintains a list of all returns and repeatedly calculates their mean. It would be more efficient to use techniques similar to those in section 2.4 to maintain just the mean and a count (for each state-action pair) and update them incrementally. Describe how the pseudocode would be altered to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the initialization :\n",
    "- Remove $Returns(s,a)$\n",
    "- Set $Q(s,a) \\leftarrow 0$ for all s,a\n",
    "- Introduce $N(s,a) \\leftarrow 0$ for all s,a <br>\n",
    "  \n",
    "In the loop :\n",
    "- $N(S_t,A_t) \\leftarrow N(S_t,A_t) + 1$\n",
    "- $Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\frac{1}{N(S_t,A_t)} [G - Q(S_t,A_t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.5**\n",
    "\n",
    "Consider an MDP with a single nonterminal state and a single action that transitions back to the non terminal state with probability $p$ and transitions to the terminal state with probability $1-p$. Let the reward be $+1$ on all tranisitions, and $\\gamma=1$. Suppose you obersve one episode that lasts 10 steps, with a return of 10. What are the first-visit and every-visit estimators of the value of the nonterminal state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the first-visit method : the estimator would be 10.\n",
    "- For the every-visit method : the estimator would be $\\frac{1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10}{10} = 5.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.6**\n",
    "\n",
    "What is the analogous to (5.6) for action values $Q(s,a)$ instead of state values $V(s)$, again given returns generated using b?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder :\n",
    "$$\n",
    "V(s) = \\frac{\\sum_{t \\in \\tau(s)} \\rho_{t:T(t)-1}G_t}{\\sum_{t \\in \\tau(s)} \\rho_{t:T(t)-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For action values, we would redefine $\\rho'$ as : \n",
    "\\begin{aligned}\n",
    "    \\rho_{t:T-1}' &= \\frac{p(S_{t+1}|S_t, A_t) \\prod_{k=t+1}^{T-1} \\pi(A_{k}|S_{k}) p(S_{k+1}|S_k, A_k)}{p(S_{t+1}|S_t, A_t) \\prod_{k=t+1}^{T-1} b(A_{k}|S_{k}) p(S_{k+1}|S_k, A_k)} \\\\\n",
    "    &= \\frac{\\prod_{k=t+1}^{T-1} \\pi(A_{k}|S_{k})}{\\prod_{k=t+1}^{T-1} b(A_{k}|S_{k})}\n",
    "\\end{aligned}\n",
    "The product's index starts at t+1 instead of t. Then :\n",
    "$$\n",
    "Q(s,a) = \\frac{\\sum_{t \\in \\tau(s,a)} \\rho_{t:T(t)-1}' G_t}{\\sum_{t \\in \\tau(s,a)} \\rho_{t:T(t)-1}'}\n",
    "$$\n",
    "where $\\tau(s,a)$ would be the set of the timesteps where the pair (s,a) has been visited (either first-visit or every-visit) <br>\n",
    "$G_t$ would be the return after time $t$, starting from $t \\in \\tau(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.7**\n",
    "\n",
    "In learning curves such as those shown in Figure 5.3 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method, error first increased then decreased. Why do you think this happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted importance sampling estimator is biased, but is asymptotically unbiased. Hence the decrease in the long run. <br> \n",
    "As for the increase at the beginning, it can't be the effect of bad luck as this the result of 100 independent runs averaged out. <br>\n",
    "My explanation for this is that at the beginning, we're averaging over very few returns, and the two policies trajectories probability haven't had enough time to differ, making the ratio $\\rho$ close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.8**\n",
    "\n",
    "The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider :\n",
    "$$\n",
    "V(s) = \\frac{\\sum_{t \\in \\tau(s)} \\rho_{t:T(t)-1}G_t}{|\\tau(s)|}\n",
    "$$\n",
    "where $\\tau(s)$ is the set of every timesteps where s is visited. <br>\n",
    "The argument about $\\mathbb{E} \\biggr[ (\\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)} G_k)^2 \\biggr]$ for any $t \\in \\tau(s)$, being infinite still aplies. Because any episode ending with the **right** action has a ratio of zero and therefore doesn't contribute to the expectation. $G_t$ can only take the values 0 or 1, so we can basically subsitute out $G_t$ by 1 and then use the same argument<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.9**\n",
    "\n",
    "Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to use the incremental implementation for sample avarages described in Section 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pseudo-code\n",
    "Loop forerever (for each episode):\n",
    "    Generate an episode following pi\n",
    "    G <- 0\n",
    "    N(s) <- 0 for all s\n",
    "    Loop for each step of the episode, t = T-1, T-2, ..., 0:\n",
    "        G <- gamma G + R_t+1\n",
    "        Unless S_t appears in S_0, S_1, ..., S_{t-1} :\n",
    "            [REMOVED] Append G to Returns(S_t)\n",
    "            [REMOVED] V(S_t) <- average(Returns(S_t))\n",
    "            N(S_t) <- N(S_t) + 1\n",
    "            V(S_t) <- V(S_t) + 1/N(S_t) (G - V(S_t))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.10**\n",
    "\n",
    "Derive the weighted-average update rule (5.8) from (5.7). Follow the pattern of the derivation of the unweighted rule (2.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "    V_n &= \\frac{\\sum_{k=1}^{n-1} W_k G_k}{\\sum_{k=1}^{n-1} W_k} = \\frac{1}{C_{n-1}} \\sum_{k=1}^{n-1} W_k G_k \\\\\n",
    "    V_{n+1} &= \\frac{1}{C_{n}} \\sum_{k=1}^{n} W_k G_k \\\\\n",
    "\\end{aligned}\n",
    "where $C_n = \\sum_{k=1}^{n} W_k$, so :\n",
    "\\begin{aligned}\n",
    "    C_n V_{n+1} &= C_{n-1}V_n + W_n G_n \\\\\n",
    "    \\Rightarrow V_{n+1} &= V_n \\frac{C_{n-1}}{C_n} + \\frac{W_n}{C_n}G_n \\\\\n",
    "    &\\stackrel{(*)}{=} V_n (1 - \\frac{W_n}{C_n}) + \\frac{W_n}{C_n}G_n \\\\\n",
    "    &= V_n +  \\frac{W_n}{C_n} (G_n - V_n)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "    \\text{where} \\ (*) &: \\frac{C_{n-1}}{C_n} + \\frac{W_n}{\\sum_{k=1}^{n} W_k} = \\frac{\\sum_{k=1}^{n-1} W_k}{\\sum_{k=1}^{n} W_k} + \\frac{W_n}{\\sum_{k=1}^{n} W_k} = 1\\\\\n",
    "    &\\Rightarrow \\frac{C_{n-1}}{C_n} = 1 - \\frac{W_n}{\\sum_{k=1}^{n} W_k} = 1 - \\frac{W_n}{C_n}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.11**\n",
    "\n",
    "In the boxed algorithm for off-policy MC control, you may been expecting the W update to have involved the importance-sampling ratio $\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$, but instead it involves $\\frac{1}{b(A_t|S_t)}$. Why is it nevertheless correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we assigned $\\pi(S_t) \\leftarrow \\argmax_{a} Q(S_t,a)$ which is a deterministic policy in state $S_t$. <br>\n",
    "Then we check If $A_t \\ne \\pi(S_t)$ then exit the inner loop, and if $A_t = \\pi(S_t)$ , update W. For the latter, $\\pi(A_t|S_t) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.12 Racetrack (programming)**\n",
    "\n",
    "Consider driving a race car\n",
    "around a turn like those shown in Figure 5.11. You want to go as fast as\n",
    "possible, but not so fast as to run off the track. In our simplified racetrack,\n",
    "the car is at one of a discrete set of grid positions, the cells in the diagram. The\n",
    "velocity is also discrete, a number of grid cells moved horizontally and vertically\n",
    "per time step. The actions are increments to the velocity components. Each\n",
    "may be changed by +1, −1, or 0 in one step, for a total of nine actions. <br>\n",
    "Both velocity components are restricted to be nonnegative and less than 5,\n",
    "and they cannot both be zero. Each episode begins in one of the randomly\n",
    "selected start states and ends when the car crosses the finish line. The rewards\n",
    "are −1 for each step that stays on the track, and −5 if the agent tries to drive\n",
    "off the track. Actually leaving the track is not allowed, but the position is\n",
    "always advanced by at least one cell along either the horizontal or vertical\n",
    "axes. With these restrictions and considering only right turns, such as shown\n",
    "in the figure, all episodes are guaranteed to terminate, yet the optimal policy\n",
    "is unlikely to be excluded. To make the task more challenging, we assume that\n",
    "on half of the time steps the position is displaced forward or to the right by\n",
    "one additional cell beyond that specified by the velocity. Apply a Monte Carlo\n",
    "control method to this task to compute the optimal policy from each starting\n",
    "state. Exhibit several trajectories following the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see Exercise12.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
