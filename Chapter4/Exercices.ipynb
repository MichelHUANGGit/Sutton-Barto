{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.1** \n",
    "\n",
    "If $π$ is the equiprobable random policy, what is $q_π(11, down)$?\n",
    "What is $q_π(7, down)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the value function $v_\\pi(s)$ associated to the equiprobable policy $\\pi$ :\n",
    "| 0.0 | -14. | -20. | -22. |\n",
    "|-----|------|------|------|\n",
    "| -14.| -18. | -20. | -20. |\n",
    "| -20.| -20. | -18. | -14. |\n",
    "| -22.| -20. | -14. | -0.0 |\n",
    "\n",
    "To compute $q_π(11, down)$ and $q_π(7, down)$, we can use the relation between $q_\\pi(s,a)$ and $v_\\pi(s)$ :\n",
    "\\begin{equation}\n",
    "    q_\\pi(s,a) = \\sum_{s',r} p(s',r|s,a) (r + \\gamma v_\\pi(s'))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, actions deterministically determine the next state (i.e. the transitions $p(s'|s,a)$ are deterministic), the rewards are also deterministic, always -1 for all non-terminal states. So, $q_\\pi(11,down)$ is simply :\n",
    "\n",
    "\\begin{aligned}\n",
    "    q_\\pi(11,down) &= p(S_T|11,down) (0 + \\gamma v_\\pi(S_T)) \\\\\n",
    "    &= 1 \\times 0 \\\\\n",
    "    &= 0\n",
    "\\end{aligned}\n",
    "\n",
    "where $S_T$ is the terminal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for $q_\\pi(7,down)$ :\n",
    "\n",
    "\\begin{aligned}\n",
    "    q_\\pi(7,down) &= p(11|7,down) (-1 + \\gamma v_\\pi(11)) \\\\\n",
    "    &= 1 \\times (-1 + (-14)) \\\\\n",
    "    &= -15\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2**\n",
    "\n",
    "Suppose a new state 15 is added to the gridworld just below\n",
    "state 13, and its actions, left, up, right, and down, take the agent to states\n",
    "12, 13, 14, and 15, respectively. Assume that the transitions from the original\n",
    "states are unchanged. What, then, is $v_π(15)$ for the equiprobable random\n",
    "policy?\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Bellman equation : \n",
    "\\begin{aligned}\n",
    "    v_\\pi(15) &= 0.25 (-1 + v_\\pi(12)) + 0.25 (-1 + v_\\pi(13)) + 0.25 (-1 + v_\\pi(14)) + 0.25 (-1 + v_\\pi(15)) \\\\\n",
    "    &= 0.25 \\times (-1 -22) + 0.25 (-1 -20) + 0.25 (-1 -14) + 0.25 (-1 + v_\\pi(15)) \\\\\n",
    "    &= -\\frac{23}{4} - \\frac{21}{4} - \\frac{15}{4} - \\frac{1}{4} + \\frac{v_\\pi(15)}{4} \\\\\n",
    "    \\Leftrightarrow \\frac{3 v_\\pi(15)}{4} &= -\\frac{60}{4} \\\\\n",
    "     \\Leftrightarrow v_\\pi(15) &= -20\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose the dynamics of state 13 are also changed, such that\n",
    "action down from state 13 takes the agent to the new state 15. What is $v_π(15)$\n",
    "for the equiprobable random policy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the bellman equation for $v_\\pi(13)$ and $v_\\pi(15)$ as a linear system of 2 equations and 2 unknowns : \n",
    "\\begin{aligned}\n",
    "    v_\\pi(13) &= \\frac{-1 + v_\\pi(12)}{4} - \\frac{-1 + v_\\pi(9)}{4} - \\frac{-1 + v_\\pi(14)}{4} - \\frac{-1 + v_\\pi(15)}{4} \\\\\n",
    "    v_\\pi(15) &= \\frac{-1 + v_\\pi(12)}{4} - \\frac{-1 + v_\\pi(13)}{4} - \\frac{-1 + v_\\pi(14)}{4} - \\frac{-1 + v_\\pi(15)}{4} \\\\\n",
    "    \\Leftrightarrow v_\\pi(13) &= -\\frac{23}{4} - \\frac{21}{4} - \\frac{15}{4} -  \\frac{-1 + v_\\pi(15)}{4} \\\\\n",
    "    v_\\pi(15) &= -\\frac{23}{4} - \\frac{-1 + v_\\pi(13)}{4} - \\frac{15}{4} -  \\frac{-1 + v_\\pi(15)}{4} \\\\\n",
    "    \\Leftrightarrow v_\\pi(13) &= -15 + \\frac{v_\\pi(15)}{4} \\\\\n",
    "    3v_\\pi(15) &= -40  + {v_\\pi(13)} \\\\\n",
    "    \\Leftrightarrow v_\\pi(13) &= -15 + \\frac{v_\\pi(15)}{4} \\\\\n",
    "    3v_\\pi(15) &= -40  - 15 + \\frac{v_\\pi(15)}{4} \\\\\n",
    "    \\Leftrightarrow v_\\pi(13) &= -15 + \\frac{v_\\pi(15)}{4} \\\\\n",
    "    \\frac{11v_\\pi(15)}{4} &= -55 \\\\\n",
    "    \\Leftrightarrow v_\\pi(13) &= -20 \\\\\n",
    "    v_\\pi(15) &= -20 \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.3**\n",
    "\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5) for\n",
    "the action-value function $q_π$ and its successive approximation by a sequence of\n",
    "functions $q_0, q_1, q_2$, . . . ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "    q_\\pi(s,a) &= \\sum_{s',r} p(s',r|s,a) \\sum_{a'} \\pi(a'|s') \\Bigr[ r+\\gamma q_\\pi(s',a') \\Bigr]\n",
    "\\end{aligned}\n",
    "\n",
    "And therefore :\n",
    "\n",
    "\\begin{aligned}\n",
    "    q_{k+1}(s,a) &= \\sum_{s',r} p(s',r|s,a) \\sum_{a'} \\pi(a'|s') \\Bigr[ r+\\gamma q_k(s',a') \\Bigr]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.4** \n",
    "\n",
    "In some undiscounted episodic tasks there may be policies\n",
    "for which eventual termination is not guaranteed. For example, in the grid\n",
    "problem above it is possible to go back and forth between two states forever.\n",
    "In a task that is otherwise perfectly sensible, $v_π(s)$ may be negative infinity\n",
    "for some policies and states, in which case the algorithm for iterative policy\n",
    "evaluation given in Figure 4.1 will not terminate. As a purely practical matter,\n",
    "how might we amend this algorithm to assure termination even in this case?\n",
    "Assume that eventual termination is guaranteed under the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas :\n",
    "- Slightly change the policy to an $\\varepsilon$-greedy policy such that it has a small chance $\\varepsilon$ to teleport randomly to another state\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.5 (programming)**\n",
    "\n",
    "Write a program for policy iteration and\n",
    "re-solve Jack’s car rental problem with the following changes. One of Jack’s\n",
    "employees at the first location rides a bus home each night and lives near\n",
    "the second location. She is happy to shuttle one car to the second location\n",
    "for free. Each additional car still costs $2, as do all cars moved in the other\n",
    "direction. In addition, Jack has limited parking space at each location. If\n",
    "more than 10 cars are kept overnight at a location (after any moving of cars),\n",
    "then an additional cost of $4 must be incurred to use a second parking lot\n",
    "(independent of how many cars are kept there). These sorts of nonlinearities\n",
    "and arbitrary dynamics often occur in real problems and cannot easily be\n",
    "handled by optimization methods other than dynamic programming. To check\n",
    "your program, first replicate the results given for the original problem. If your\n",
    "computer is too slow for the full problem, cut all the numbers of cars in half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
