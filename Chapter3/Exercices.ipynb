{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1** \n",
    "\n",
    "Devise three example tasks of your own that fit into the reinforcement learning framework, identifying for each its states, actions, and\n",
    "rewards. Make the three examples as different from each other as possible.\n",
    "The framework is abstract and flexible and can be applied in many different\n",
    "ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An RL agent that plays Pokemon, say Pokemon Emerald gameboy version. This would correspond to a infinite MDP.**\n",
    "- Its states would be the image of the gameboy screen at a given time (which is very large)\n",
    "- The actions are the buttons on the gameboy\n",
    "- And the rewards would be whatever we decide them to be (e.g. +10 for completing an arena, +1 for beating a pokemon, -5 for losing ...)\n",
    "\n",
    "The actions only depend on the current state.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The case of a Retail store, the agent is in charge of ordering stocks and maintaining the warehouse :** <br>\n",
    "  - The states could be the observation of the demand, say every hour, and the current stock available (both in the warehouse and on sale). It could also contain information about the day of the week and the hour itself. <br>\n",
    "  - The actions would be how much stock the agent orders for each type of item the store sells. <br>\n",
    "  - The rewards would be benefits/losses of selling and ordering stocks. <br>\n",
    "\n",
    "It is almost an MDP, because the actions the agent selects mostly depend on the current state, but one could argue that it could also depend on the past. If there was an enormously large demand the last 48 hours, maybe this information could be useful to decide what to do next ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2** \n",
    "\n",
    "Is the reinforcement learning framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3** \n",
    "\n",
    "Consider the problem of driving. You could define the actions\n",
    "in terms of the accelerator, steering wheel, and brake, that is, where your\n",
    "body meets the machine. Or you could define them farther out—say, where\n",
    "the rubber meets the road, considering your actions to be tire torques. Or\n",
    "you could define them farther in—say, where your brain meets your body, the\n",
    "actions being muscle twitches to control your limbs. Or you could go to a\n",
    "really high level and say that your actions are your choices of where to drive.\n",
    "What is the right level, the right place to draw the line between agent and\n",
    "environment? On what basis is one location of the line to be preferred over\n",
    "another? Is there any fundamental reason for preferring one location over\n",
    "another, or is it a free choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It moslty depends on what our goal is when formalizing the problem :\n",
    "- If our goal was to create autonomous cars without an actual robot controlling the wheel, then we could define the actions as in the embodiment of the car. That means the agent could take control of the car's engine to accelerate, decelerate, rotate the tires etc... by accessing the engine directly, not by using the brake/accelerator/steering wheel.\n",
    "- If our goal is to create a robot that drives autonomously a car, then we could define the actions as all that control the robot's limbs, and therefore movements.\n",
    "- On the other hand, a GPS simply dictates on a higher level what path the car should follow.\n",
    "\n",
    "Where to place that line also depends on the complexity of solving the corresponding reinforcement learning problem. A GPS's decisions is certainly simpler to learn than learning to drive an autonomous car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.4** \n",
    "\n",
    "Suppose you treated pole-balancing as an episodic task but\n",
    "also used discounting, with all rewards zero except for −1 upon failure. What\n",
    "then would the return be at each time? How does this return differ from that\n",
    "in the discounted, continuing formulation of this task?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return would be $-\\gamma^\\tau$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
