{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.1**\n",
    "\n",
    "If V changes during the episode, then (6.6) only holds approximately; what would the difference be between the two sides? Let $V_t$ denote the array of state vlaues used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "\\begin{aligned}\n",
    "    (6.2) \\ \\ &V_{t+1}(S_t) = V_t(S_t) + \\alpha \\delta_t\\\\\n",
    "    &V_t(S_t) = V_{t+1}(S_t) - \\alpha \\delta_t \\\\\n",
    "    (6.5) \\ \\ &\\delta_t = R_{t+1} - \\gamma V_t(S_{t+1}) - V_t(S_t)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "    G_t - V_t(S_t) &= R_{t+1} + \\gamma G_{t+1} - V_t(S_t) \\\\\n",
    "    &= R_{t+1} + \\gamma V_t(S_{t+1}) - V_t(S_t) + \\gamma (G_{t+1}-V_t(S_{t+1})) \\\\\n",
    "    &= \\delta_t + \\gamma (G_{t+1}-V_t(S_{t+1})) \\\\\n",
    "    &= \\delta_t + \\gamma (G_{t+1}-V_{t+1}(S_{t+1})) +  \\gamma \\alpha \\delta_t \\\\\n",
    "    &= \\delta_t (1 + \\gamma \\alpha) + \\gamma \\bigr[ \\delta_{t+1} + \\gamma (G_{t+2} - V_{t+2}(S_{t+2})) + \\gamma \\alpha \\delta_{t+1} \\bigr] \\\\\n",
    "    &= \\delta_t (1 + \\gamma \\alpha) + \\gamma \\delta_{t+1} (1 + \\gamma \\alpha) + \\gamma^2 (G_{t+2} - V_{t+2}(S_{t+2})) \\\\ \n",
    "    &= \\delta_t (1 + \\gamma \\alpha) + \\gamma \\delta_{t+1} (1 + \\gamma \\alpha) + \\cdots + \\gamma^{T-t-1} \\delta_{T-1} (1 + \\gamma \\alpha) + \\gamma^{T-t} (G_T - V_T(S_T)) \\\\\n",
    "    &= (1 + \\gamma \\alpha) \\sum_{k=0}^{T-t-1} \\gamma^k \\delta_{t+k} \\\\\n",
    "    &= (1 + \\gamma \\alpha) \\sum_{k=t}^{T-1} \\gamma^{t-k} \\delta_{k}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (6.6) for a non constant array V changes by a small term, that is by adding $\\gamma \\alpha$ times the error. If $\\alpha$ is small, then this is almost equivalent to having a constant array V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.2**\n",
    "\n",
    "This is an exercise to help develop your intuition about why TD methods are often more efficient than Monte Carlo methods. Consider the driving home example and how it is adressed by TD and Monte Carlo methods. Can you imagine a scenario in which a TD update would be better on average than a Monte Carlo update? Give an example scenario--a description of past experience and a current state in which you would expect the TD update to be better. <br> Here's a hint: Suppose you have lots of experience driving home from work. Then you move to a new building a new parking lot (but you still enter the highway at the same place). Can you see why TD updates are likely to be much better, at least initially, in this case? Might the same sort of thing happen in the original scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "TD methods rely on previous estimates, and since we assume to have a lot of experience driving from home, i.e. our value estimates for states from work to home are good estimates, TD methods are going to work better: <br>\n",
    "If we were to change building, and thus the parking lot (the starting point to our way home), we would have to learn value estimates for the states specific for this new parking lot. But because we still enter the highway like we used to and we have a good estimate of the value for the state \"Entered the highway\", then the TD method we will easily form an estimate of the value of the state right before \"Entered the highway\". Indeed, the TD method would use our previously learned value of \"Entered the highway\" to learn the value of the state right before, whereas the Monte Carlo method would not use that previously known estimate and wait until the end of the episode to learn. Even if we changed our initial starting point, tt is pretty intuitive that we would want to use our previous estimate of remaining time to go home at \"Entered the highway\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.3**\n",
    "\n",
    "From the results shown in the left graph of the random walk example it appears that the first episode results in a change in only $V(A)$. What does this tell you about what happened on the first episode? Why was only the estimate for this one state changed? By exactly how much was it changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "The only way only one value estimate is changed starting from A, is that the episode terminated after one step. That means the episode started at A and went left and ended. If it went right instead of left, then on the next step, the value of B (and possibly other states) would have been changed. The TD(0) update goes: \n",
    "\\begin{aligned}\n",
    "V(A) &\\leftarrow V(A) + \\alpha  \\bigr[R_1 + \\gamma V(S_L) - V(A) \\bigr] \\\\\n",
    "&\\leftarrow 0.5 + 0.10  \\bigr[0 + 1 \\times 0 - 0.5 \\bigr] \\\\\n",
    "&\\leftarrow 0.45\n",
    "\\end{aligned}\n",
    "where $V(S_L)$ is the value of the terminal left state, so 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.4**\n",
    "\n",
    "The specific results shown in the right graph of the random walk example are dependent on the value of the step-size parameter, $\\alpha$. Do you think the conclusions about which algorithm is beter would be affected if a wider range of $\\alpha$ values were used? Is there a different, fixed value of $\\alpha$ at which either algorithm would have performed significantly better than shown? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Choosing the constant step-size $\\alpha$ is a question of trade-off between:<br>\n",
    "- Option (1), learning faster at the beginning but not getting close enough to the solution in the long run\n",
    "- Option (2), learning slower at the beginning but getting closer to the solution in the long run\n",
    "  \n",
    "As shown in the figure, higher values of $\\alpha$ are leaning more towards option (1) and lower values of $\\alpha$ towards option (2). Ultimately for a given algorithm, there is not a fixed value of $\\alpha$ that would perform significantly better than all other values of $\\alpha$, in the sense of Option (1) and Option (2). There is always a trade-off between both options. <br>\n",
    "In order to make a fair comparison between MC and TD, we should compare them with their best respective step-size $\\alpha$. However, a wider range of $\\alpha$ values would not have changed the conclusions here, because for MC, the highest value of $\\alpha$ is 0.04 and its RMSE stabilizes at 0.12-ish. According to the trade-off logic between Options (1) and (2), we could easily interpolate what would happen for a wider range of $\\alpha$:\n",
    "- For $\\alpha > 0.04$ on MC, it would prioritize Option (1) compared to $\\alpha = 0.04$, so it wouldn't do any better than 0.12 RMSE. And thus, it would not be better than any value of $\\alpha$ for TD.\n",
    "- For $\\alpha < 0.01$ on MC, it would prioritize Option (2) compared to $\\alpha = 0.01$, the RMSE might go below the $\\alpha = 0.01$ one after 100+ steps, but it would still be worse than $\\alpha = 0.05$ on TD.\n",
    "\n",
    "TD clearly beats MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.5**\n",
    "\n",
    "In the right graph of the random walk example, the RMS error of the TD method seems to go down and then up again, particularly at high $\\alpha$'s. What could have caused this? Do you think this always occurs, or might it be a function of how the approximate value function was initialized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.6**\n",
    "\n",
    "In Example 6.2, we stated that the true values for the random walk example are $\\frac{1}{6}$, $\\frac{2}{6}$, $\\frac{3}{6}$, $\\frac{4}{6}$ and $\\frac{5}{6}$, for states A through E. Describe at least two different ways that these could have been computed. Which would you guess we actually used? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "1. Through a DP method for prediction, like value iteration where the policy is 50% go left, 50% go right.\n",
    "2. By solving Bellman's equation directly with matrix inversion, since there are no actions:\n",
    "\\begin{aligned}\n",
    "    v = r + Pv\n",
    "\\end{aligned}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{aligned}\n",
    "v =\n",
    "\\begin{bmatrix} \n",
    "        v(S_L) \\\\\n",
    "        v(A) \\\\\n",
    "        v(B) \\\\\n",
    "        v(C) \\\\\n",
    "        v(D) \\\\\n",
    "        v(E) \\\\\n",
    "        v(S_R) \n",
    "\\end{bmatrix}\n",
    "r = \n",
    "\\begin{bmatrix} \n",
    "        0 \\\\\n",
    "        0 \\\\\n",
    "        0 \\\\\n",
    "        0 \\\\\n",
    "        0 \\\\\n",
    "        0.5 \\\\\n",
    "        0 \n",
    "\\end{bmatrix}\n",
    "P = \n",
    "\n",
    "\\begin{bmatrix} \n",
    "        P(S_L \\rightarrow S_L) & P(S_L \\rightarrow A) & \\dots & P(S_L \\rightarrow S_R) \\\\\n",
    "        P(A \\rightarrow S_L) & P(A \\rightarrow A) & \\dots & P(A \\rightarrow S_R) \\\\\n",
    "        \\dots & \\dots & \\dots & \\dots \\\\\n",
    "        P(E \\rightarrow S_L) & P(E \\rightarrow A) & \\dots & P(E \\rightarrow S_R) \\\\\n",
    "        P(S_L \\rightarrow S_L) & P(S_L \\rightarrow A) & \\dots & P(S_L \\rightarrow S_R) \\\\\n",
    "\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\n",
    "\\begin{bmatrix} \n",
    "        0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "        0.5 & 0 & 0.5 & 0 & 0 & 0 & 0 \\\\\n",
    "        0 & 0.5 & 0 & 0.5 & 0 & 0 & 0 \\\\\n",
    "        0 & 0 & 0.5 & 0 & 0.5 & 0 & 0 \\\\\n",
    "        0 & 0 & 0 & 0.5 & 0 & 0.5 & 0 \\\\\n",
    "        0 & 0 & 0 & 0 & 0.5 & 0 & 0.5 \\\\\n",
    "        0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v$ is the state values, $r$ is the average immediate reward for each state, $P$ is the transition matrix, the terminal states can't transition. We can solve for v:\n",
    "\\begin{aligned}\n",
    " &(I-P) v = r \\\\\n",
    " & v = (I-P)^{-1}r\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0.5, 0. , 0.5, 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.5, 0. , 0.5, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.5, 0. , 0.5, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0.5, 0. , 0.5, 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0.5, 0. , 0.5],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.array([\n",
    "    [0,  0  ,  0,0  ,0  ,0  ,0  ],\n",
    "    [0.5,0  ,0.5,0  ,0  ,0  ,0  ],\n",
    "    [0  ,0.5,0  ,0.5,0  ,0  ,0  ],\n",
    "    [0  ,0  ,0.5,0  ,0.5,0  ,0  ],\n",
    "    [0  ,0  ,0  ,0.5,0  ,0.5,0  ],\n",
    "    [0,  0  ,  0,0  ,0.5,0  ,0.5],\n",
    "    [0,  0  ,  0,0  ,0  ,0  ,0  ],\n",
    "])\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0. ],\n",
       "       [0.5],\n",
       "       [0. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0.5],\n",
    "    [0]\n",
    "])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.identity(7)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.16666667],\n",
       "       [0.33333333],\n",
       "       [0.5       ],\n",
       "       [0.66666667],\n",
       "       [0.83333333],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(I-P) @ r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably used solution 2, because it gives an exact analytical solution (it can be solved manually to get the exact result, but here we used numpy's library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.7**\n",
    "\n",
    "Design an off-policy version of the TD(0) update that can be used with an arbitrary target policy $\\pi$ and covering behavior policy $b$, using at each step $t$ the importance sampling ratio $\\rho_{t:t}$ (5.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "in the loop of each episode step:\n",
    "\\begin{aligned}\n",
    "    &\\text{For each step } t=0,1,...,T \\\\\n",
    "    & \\ \\ \\ \\ \\ A \\leftarrow \\text{action given by $\\pi$ for $S$} \\\\\n",
    "    & \\ \\ \\ \\ \\ \\text{Take action A, observe $R$, $S'$} \\\\\n",
    "    & \\ \\ \\ \\ \\ V(S) \\leftarrow V(S) + \\alpha  \\bigr[\\gamma V(S') - V(S) \\bigr]  + \\alpha \\rho_{t:t}R \\\\\n",
    "    & \\ \\ \\ \\ \\ S \\leftarrow S'\n",
    "\\end{aligned}\n",
    "\n",
    "because:\n",
    "\\begin{aligned}\n",
    "    &v_\\pi(S_t) = \\mathbb{E}_{\\pi} [R_{t+1}] + \\gamma v_\\pi(S_{t+1}) \\\\\n",
    "    &v_\\pi(S_t) = \\mathbb{E}_{b} [\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} R_{t+1}]  + \\gamma v_\\pi(S_{t+1}) \\\\\n",
    "    &v_\\pi(S_t) = \\mathbb{E}_{b} [\\rho_{t:t} R_{t+1}]  + \\gamma v_\\pi(S_{t+1})\n",
    "\\end{aligned}\n",
    "and\n",
    "\\begin{aligned}\n",
    "    \\mathbb{E}_b \\bigr[\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} R_{t+1} \\bigr] &= \\mathbb{E}_b \\bigr[ \\mathbb{E}_b[ \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} R_{t+1} | S_t] \\bigr]\\\\\n",
    "    &=\\mathbb{E}_b \\bigr[ \\sum_a \\frac{\\pi(a|S_t)}{b(a|S_t)} b(a|S_t) \\sum_r r(S_t,a) \\bigr] \\\\\n",
    "    &= \\mathbb{E}_\\pi \\bigr[ \\mathbb{E}_\\pi[R_{t+1} | S_t] \\bigr] \\\\\n",
    "    &= \\mathbb{E}_\\pi \\bigr[ R_{t+1} \\bigr]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.8**\n",
    "\n",
    "Show that an action-value version of (6.6) holds for the action-value form of the TD error $\\delta_t = R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "\\begin{aligned}\n",
    "    G_t - Q(S_t,A_t) &= R_{t+1} + \\gamma G_{t+1} - Q(S_t,A_t) \\\\\n",
    "    &= R_{t+1}  + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) + \\gamma (G_{t+1} -Q(S_{t+1},A_{t+1})) \\\\\n",
    "    &= \\delta_t + \\gamma (G_{t+1}-Q(S_{t+1},A_{t+1})) \\\\\n",
    "    &= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+1}-V_{t+1}(S_{t+1})) \\\\\n",
    "    &= \\delta_t + \\gamma \\delta_{t+1} + \\cdots + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_T - V_T(S_T)) \\\\\n",
    "    &= \\sum_{k=t}^{T-1} \\gamma^{t-k} \\delta_{k}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.9-6.10 (Windy Gridworld)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see Exercise9-10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.11**\n",
    "\n",
    "Why is Q-Learning considered an off-policy control method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Because it learns the optimal policy no matter the behavioral policy followed, e.g. an $\\varepsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.12**\n",
    "\n",
    "Suppose action selection is greedy. Is Q-Learning then exactly the same algorithm as Sarsa? Will they make exactly the same action selection and weight updates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "If the action selection were greedy for both SARSA and Q-Learning, then in the SARSA algorithm, we'd choose $A'$ greedily, thus making the following update: $Q(S,A) \\leftarrow Q(S,A) + \\alpha \\bigr[ R + \\gamma Q(S',A') - Q(S,A) \\bigr]$ where $A' = max_a Q(S,a)$ which ends up being the same update as Q-Learning. Then on the next iteration, both algorithms would select the same action greedily because the Q table is the same, and update weight exactly the same, then choose again the same action greedily, and so on and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.13**\n",
    "\n",
    "What are the update operations for Double Expected Sarsa with an $\\varepsilon$-target policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.14**\n",
    "\n",
    "Describe how the task of Jack's Car Rental (Example 4.2) could be reformulated in terms of afterstates. Why, in terms of this specific task, would such reformulation be likely to speed convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Consider the state with 10 cars on location1 and 10 cars on location2. Moving n cars from location1 to location2 and m cars from location2 to location1 for n,m in $\\set{1,2,3,4,5}$ is equivalent to moving n-m cars from location1 to location2. Some actions lead to the same afterstate. Such reformulation would therefore reduce the number of states to be learned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
